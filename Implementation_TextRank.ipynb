{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab4e290",
   "metadata": {},
   "source": [
    "# Spatio-Temporal Analysis of Covid-19 related Tweets using a modified TextRank algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9250c",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "## Objective\n",
    "The primary objective of this project is to analyze six months of Twitter data related to COVID-19, spanning from April to September 2020, with an average daily volume close to one million tweets. The focus is on establishing meaningful trends and understanding how interest in specific topics has evolved globally during this period. The chosen approach employs a customized version of TextRank for extractive summarization.\n",
    "\n",
    "## Text Summarization Categories\n",
    "In the realm of text summarization, two main categories exist: \n",
    "1. **Extractive Summarization:** Involves selecting a subset of sentences from the original text.\n",
    "2. **Abstractive Summarization:** Expresses the document's ideas using different wording.\n",
    "\n",
    "For this project, the emphasis lies on extractive summarization.\n",
    "\n",
    "## Modified TextRank Algorithm\n",
    "The modified TextRank algorithm draws inspiration from the PageRank algorithm, wherein important web pages are linked to other important web pages. Similarly, our approach assumes that important sentences are linked or similar to other important sentences in the input document. The algorithm constructs a similarity graph where each vertex represents a sentence vector, and edge weights denote their similarity. Edges are established between vertices if the similarity measure exceeds a predefined threshold, effectively removing less coherent nodes (500 nodes removed).\n",
    "\n",
    "An innovation in this modification involves introducing a damping factor to the algorithm. The formula incorporates TextRank of the sentence for a given node and the degree of that node. A presentation and paper have been included to enhance understanding.\n",
    "\n",
    "## Data Preprocessing\n",
    "The initial step involved collecting English tweets related to COVID-19 from April to September 2020. Subsequently, preprocessing steps included removing hashtags, URLs, emoticons, and irrelevant content.\n",
    "\n",
    "## Keyword Classification\n",
    "Simultaneously, two processes were executed:\n",
    "1. The first utilized the entire six months' data to generate keywords above a specified frequency, which were then classified and associated with six different topics or buckets. For instance, keywords like \"government\" and \"leadership\" were classified under the administration topic, while \"virus\" and \"cured\" belonged to the disease topic.\n",
    "\n",
    "2. The second process involved applying the modified TextRank algorithm to generate summary files for each of the 183 days. Within these summaries, keywords were identified, and the associated topic counts were incremented, resulting in 183 data points.\n",
    "\n",
    "## Data Normalization and Visualization\n",
    "These data points were normalized by dividing them by the number of tweets on that particular day. The final normalized data points were plotted, and a 7-day moving average was applied for enhanced readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9e9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25b7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "import re\n",
    "import urllib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import wordnet\n",
    "from collections import namedtuple\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import operator\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "\n",
    "import string\n",
    "#import preprocessor as p\n",
    "#from preprocessor.api import clean, tokenize, parse\n",
    "import unidecode\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5831435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3799792",
   "metadata": {},
   "source": [
    "The below keywords are generated by parsing the data, looking for words above a specified frequency. While we generate 7 buckets, ultimately due to the lack of data points, the religion bucket is dropped. \n",
    "\n",
    "One look at the list of keywords would show that a lot of them are abbreviations, Hindi words typed out in English, or just typos. While we do not expect the final trends to change significantly, for the sake of completeness, we keep working on bettering these keyword lists. \n",
    "\n",
    "If the reader wishes to contribute, they could do so by simply adding new relevant keywords or other variations of existing keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e63c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "administration = ['relief', 'lie', 'pulis', 'srkaar', 'congress', 'police', 'pm', 'chief', 'minister', 'hm', 'members', 'distributed', 'govt', 'government', 'suppo', 'food', 'judgement', 'distributing', 'diyaa', 'modi', 'scandalous', 'cm', 'kovind', 'mjduur', 'food,', 'presidents', 'nehru', 'dynasty', 'attacks', 'opponents', 'leadership', 'narendra', 'prime', 'bjp','aadesh','amendment','ordinance,','commitment','raashn','express','scam','niti','president','modis','fund','ruupaay','producer','constable','maansiktaa','crore']\n",
    "\n",
    "disease = ['manifests','covid19', 'corona', 'coronavirus', 'cases', 'koronaa', 'positive', 'patients', 'covid', 'pandemic', 'crisis', 'koroonnnaa', 'spread', 'dies', 'virus', 'cured', 'donate', 'diseases', 'maut', 'suffering', 'deaths', 'died','epidemic','death','patient','demise','case','dead','coronavirus','maaro', 'tested', 'fighting','symptoms']\n",
    "\n",
    "healthcare = ['help', 'nivaarnn', 'humanitarian','aid','treatment', 'ilaaj', 'metabolics', 'stitched', 'labs', 'blood', 'fight', 'rapid', 'stepped', 'respect', 'healthcare', 'plasma', 'icmr', 'hospitals', 'donated', 'wellbeing','commend','lose','save','lives','hospital','dr','doctor','antivenom','treatment','poisonous','medicines','donating','medical','testing','giving','gratitude','villagedoctors']\n",
    "\n",
    "location = ['india','delhi', 'desh', 'world', 'dillii', 'mumbai', 'indias', 'tamil', 'country', 'china', 'states', 'state', 'tmilnaaddu', 'manipurdignity','area','maharashtra','chennai','nadu','yuupii','gaajiyaabaad','amerikn','countries','nizamuddin','bihar,','central','western','karnataka','south']\n",
    "\n",
    "precaution = ['proactive','masks', 'test', 'kits', 'face', 'kit', 'kitt', 'walking', 'health', 'measures', 'safety,','mask','purchase', 'ghr','lockdown', 'home', 'month', 'stay', 'lonkddaaun']\n",
    "\n",
    "public = ['office','people', 'workers', 'indian', 'log', 'lady', 'bhuukhe', 'everyone', 'youth', 'girl', 'human', 'krodd', 'media', 'migrant', 'thalapathy', '12year', 'kmaane', 'shrii', 'actor', 'worker', 'khaanaa', 'lakhs', 'millions', 'appeals','news','everyones','brothers','person','protect','journalists','sisters,','needy','body','private','sir','community','neighbors','shops','privaar','relatives','family']\n",
    "\n",
    "religion = ['muslims','holy', 'muslim', 'jamaat', 'jaatii', 'mubarak', 'hinduu', 'markaz', 'mohammad','jamaats','pray','msjidon','mndiron','hindu','ramzan','priests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f60ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"all_months.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcaa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"all_months_normalized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac144d",
   "metadata": {},
   "source": [
    "The dataset we are dealing with is a massive 185 million tweets which goes in the order of Gigabytes. The actual dataset is stored in a Google Drive, with a link attached in the README file. \n",
    "\n",
    "Below is a tutorial on how to run our modified TextRank algorithm for the month of September. It is also worth noting that due to a lack of compute (the generation of similarity matrix makes the space $ O(n^2) $ ), we were unable to run our modified algorithm on the entire day all at once. Instead, we broke the text down into 4 parts, ran the algorithm on each part to generate 4 summaries, and combined them all to generate a final summary. \n",
    "\n",
    "First we remove some irrelevant literature, like stopwords, emoticons, hashtags, etc. We then generate the four summaries and the main summary. In the main summary, we try to search for our keywords, and assign the generate bucket counts for each day. This process is repeated for all 185 days, generating 185 dataponts, through which we generate our bucket trends. \n",
    "\n",
    "An important point to note was that interest in Covid-19 also did not stay constant, it ultimately started dying down. So a count of 3000 out of 1 million tweets could not be held to the same standard as a count of 3000 in 500k tweets. To account for this inconsistency, we normalize our bucket counts, dividing them by the daily tweet counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94463180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for ww in range (1,31):    \n",
    "    print(ww)\n",
    "    df = pd.read_csv(str(ww) + \"Sep_text_filtered.csv\")\n",
    "    df = df['text']\n",
    "    a = len(df)//4\n",
    "    b = len(df)//2\n",
    "    c = 3*a\n",
    "    d = len(df)\n",
    "\n",
    "    text1 =''\n",
    "    for i in range(0, a):\n",
    "        \n",
    "        if type(df[i]) == str:\n",
    "            text1 = text1 + df[i]\n",
    "\n",
    "    text2 =''\n",
    "    for i in range(a+1, b):\n",
    "        \n",
    "        if type(df[i]) == str:\n",
    "            text2 = text2 + df[i]\n",
    "\n",
    "    text3 =''\n",
    "    for i in range(b+1, c):\n",
    "        \n",
    "        if type(df[i]) == str:\n",
    "            text3 = text3 + df[i]\n",
    "    text4 =''\n",
    "    for i in range(c+1, d):\n",
    "        \n",
    "        if type(df[i]) == str:\n",
    "            text4 = text4 + df[i]\n",
    "\n",
    "    texts = [text1, text2, text3, text4]\n",
    "    for k in range(0,len(texts)):\n",
    "        input_file1=texts[k]     #opening the original dataset\n",
    "\n",
    "        data=\"\"                 #storing the dataset as a string\n",
    "        for x in input_file1:\n",
    "            data=data+x\n",
    "        data = unidecode.unidecode(data)    #to remove accents in the string\n",
    "        #print(data)\n",
    "        #clean_text=p.clean(data)            #to clean the data by removing tweet ids and labels\n",
    "\n",
    "        #####################\n",
    "\n",
    "        data = re.sub(r'#\\w*','',data) # Remove hashtag\n",
    "        data = re.sub(r'(RT|rt|FAV|fav|VIA|via)','',data) #Remove twitter reserved words\n",
    "\n",
    "        def get_url_patern():\n",
    "            return re.compile(\n",
    "                r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "                r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "\n",
    "        def remove_urls(self):\n",
    "                self = re.sub(pattern=get_url_patern(), repl='', string=self)\n",
    "                return self\n",
    "\n",
    "        TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "        def remove_tags(text):\n",
    "            return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "        data = remove_urls(data) #remove urls\n",
    "        data = remove_tags(data) #remove tags\n",
    "        data = re.sub(r'\\.+', \".\", data) #remove multiple dots\n",
    "        data = data.replace('[', '')\n",
    "        data = data.replace(']', '')\n",
    "        data = data.replace('|', '')\n",
    "\n",
    "        #####################\n",
    "\n",
    "\n",
    "        import urllib.parse as urlparse\n",
    "        #>>> string = '@peter I really love that shirt at #Macy. http://bit.ly//WjdiW#'\n",
    "        new_string = ''\n",
    "        for i in data.split():\n",
    "            s, n, p, pa, q, f = urlparse.urlparse(i)\n",
    "            if s and n:\n",
    "                pass\n",
    "            elif i[:1] == '@':\n",
    "                pass\n",
    "            elif i[:1] == '#':\n",
    "                new_string = new_string.strip() + ' ' + i[1:]\n",
    "            else:\n",
    "                new_string = new_string.strip() + ' ' + i\n",
    "\n",
    "        clean_text=new_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"\\n\\n\\n\\n\\n\", clean_text)\n",
    "        bad_chars = [';', ':', '!', \"relevant\" , \"+\" , \"-\" ,  \"not_relevant\" , \"*\", \"RT\", \"0U\" , \"https\", \"http\", \".html\",\"/\" , '@' , \"?\" , \"(\"  ,  \")\" , \"^\" , \"$\" , \"&\" , \"_\" , \"%\"]  #to remove the irrelevant characters\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        emoticons_happy = set([\n",
    "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "            '<3'\n",
    "            ])\n",
    "\n",
    "        # Sad Emoticons\n",
    "        emoticons_sad = set([\n",
    "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "            ':c', ':{', '>:\\\\', ';('\n",
    "            ])\n",
    "\n",
    "        #combine sad and happy emoticons\n",
    "        emoticons = list(emoticons_happy.union(emoticons_sad))\n",
    "\n",
    "        for i in bad_chars or emoji_pattern or emoticons:       #clean_text contains the final cleaned data\n",
    "            clean_text= clean_text.replace(i, '')  \n",
    "        #print(clean_text)\n",
    "\n",
    "        f20 = open(str(k) + '.txt','w')     #open a file to store the cleaned data which is the input for the extractive summarization\n",
    "        for i in clean_text:\n",
    "            f20.write(i)\n",
    "        f20.close()\n",
    "\n",
    "\n",
    "    r = ''\n",
    "    l = ['0','1','2','3']\n",
    "\n",
    "\n",
    "\n",
    "    for k in l:\n",
    "        def textrank(document):\n",
    "            sentence_tokenizer = PunktSentenceTokenizer()\n",
    "            sentences = sentence_tokenizer.tokenize(document)\n",
    "            f = open('sent_tokenized_file.txt','w+')\n",
    "            f1 = open('processed_file.txt','w')\n",
    "            #print sentences\n",
    "            \n",
    "            #sentence index tagging\n",
    "            sentence_mod=[]\n",
    "            processed_sent=[]\n",
    "            index=0\n",
    "            eol = '\\n'\n",
    "            \n",
    "            for item in sentences:\n",
    "                lineitem = str(index)+' '+item+eol\n",
    "                f.writelines(str(lineitem)) \n",
    "                new_item=remove_stopword(item)   #remove stopwords\n",
    "                f1.writelines((str(new_item)+eol)) \n",
    "                index=index+1\n",
    "                item += u' '\n",
    "                item +=str(index)\n",
    "                sentence_mod.append(item)\n",
    "                \n",
    "                new_item +=u' '\n",
    "                new_item +=str(index)\n",
    "                processed_sent.append(new_item)\n",
    "            f.close()\n",
    "            \n",
    "            f1.close\n",
    "            #print sentence_mod\n",
    "            #print processed_sent\n",
    "            bow_matrix = CountVectorizer().fit_transform(processed_sent)\n",
    "            \n",
    "            #print bow_matrix\n",
    "            \n",
    "            #f1.write(str(bow_matrix))\n",
    "            normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    "            \n",
    "            #print normalized\n",
    "        \n",
    "            similarity_graph = normalized * normalized.T\n",
    "        \n",
    "            nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "            scores = nx.pagerank(nx_graph)\n",
    "            #nx.draw(nx_graph)\n",
    "            #plt.show()\n",
    "            return sorted(((scores[i],s) for i,s in enumerate(processed_sent)),\n",
    "                        reverse=True),sentence_mod\n",
    "\n",
    "        input_file=open(k+ '.txt', 'r')\n",
    "        title=input_file.read()\n",
    "        \n",
    "        #print title\n",
    "        txt_rnk,sentence_mod=textrank(title)\n",
    "        #print sentence_m\n",
    "        length = len(txt_rnk)\n",
    "        #print(length)\n",
    "        n=int(length*0.25)\n",
    "        #print \"n:------------------->\",n\n",
    "        i=0\n",
    "        result=[]\n",
    "        while i< n:\n",
    "            result.append(txt_rnk[i][1])   \n",
    "            i=i+1\n",
    "            \n",
    "        result.sort(key = lambda x: int(x.rsplit(' ',1)[1]))# sort according sentence index #\n",
    "\n",
    "        result_sent=''\n",
    "        result_index=[]\n",
    "        for item in result:\n",
    "            for k in sentence_mod:\n",
    "                if item.rsplit(None, 1)[-1]==k.rsplit(None, 1)[-1]:\n",
    "                    result_sent+=k.rsplit(' ', 1)[0] #remove sentence index from the result #\n",
    "                    result_index.append(k.rsplit(' ', 1)[1])\n",
    "\n",
    "        r = r+result_sent\n",
    "        #print(len(r))\n",
    "\n",
    "\n",
    "    def textrank(document):\n",
    "        sentence_tokenizer = PunktSentenceTokenizer()\n",
    "        sentences = sentence_tokenizer.tokenize(document)\n",
    "        f = open('sent_tokenized_file.txt','w+')\n",
    "        f1 = open('processed_file.txt','w')\n",
    "        #print sentences\n",
    "        \n",
    "        #sentence index tagging\n",
    "        sentence_mod=[]\n",
    "        processed_sent=[]\n",
    "        index=0\n",
    "        eol = '\\n'\n",
    "        \n",
    "        for item in sentences:\n",
    "            lineitem = str(index)+' '+item+eol\n",
    "            f.writelines(str(lineitem)) \n",
    "            new_item=remove_stopword(item)   #remove stopwords\n",
    "            f1.writelines((str(new_item)+eol)) \n",
    "            index=index+1\n",
    "            item += u' '\n",
    "            item +=str(index)\n",
    "            sentence_mod.append(item)\n",
    "            \n",
    "            new_item +=u' '\n",
    "            new_item +=str(index)\n",
    "            processed_sent.append(new_item)\n",
    "        f.close()\n",
    "        \n",
    "        f1.close\n",
    "        #print sentence_mod\n",
    "        #print processed_sent\n",
    "        bow_matrix = CountVectorizer().fit_transform(processed_sent)\n",
    "        \n",
    "        #print bow_matrix\n",
    "        \n",
    "        #f1.write(str(bow_matrix))\n",
    "        normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    "        \n",
    "        #print normalized\n",
    "    \n",
    "        similarity_graph = normalized * normalized.T\n",
    "    \n",
    "        nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        #nx.draw(nx_graph)\n",
    "        #plt.show()\n",
    "        return sorted(((scores[i],s) for i,s in enumerate(processed_sent)),\n",
    "                    reverse=True),sentence_mod\n",
    "\n",
    "    title=r\n",
    "    \n",
    "    #print title\n",
    "    txt_rnk,sentence_mod=textrank(title)\n",
    "    #print sentence_m\n",
    "    length = len(txt_rnk)\n",
    "    #print(length)\n",
    "    n=int(length*0.1)\n",
    "    #print \"n:------------------->\",n\n",
    "    i=0\n",
    "    result=[]\n",
    "    while i< n:\n",
    "        result.append(txt_rnk[i][1])   \n",
    "        i=i+1\n",
    "        \n",
    "    result.sort(key = lambda x: int(x.rsplit(' ',1)[1]))# sort according sentence index #\n",
    "\n",
    "    result_sent=''\n",
    "    result_index=[]\n",
    "    for item in result:\n",
    "        for k in sentence_mod:\n",
    "            if item.rsplit(None, 1)[-1]==k.rsplit(None, 1)[-1]:\n",
    "                result_sent+=k.rsplit(' ', 1)[0] #remove sentence index from the result #\n",
    "                result_index.append(k.rsplit(' ', 1)[1])\n",
    "\n",
    "    f20 = open(str(ww) + 'Sep.txt','w')   \n",
    "\n",
    "    f20.write(result_sent)\n",
    "    f20.close()\n",
    "\n",
    "    f2 = open(str(ww)+ \"Sep.txt\", 'r', encoding=\"utf-8\")\n",
    "    a=f2.read()\n",
    "    f2.close()\n",
    "\n",
    "    l = re.split('(\\W+?)', a)\n",
    "    words = []\n",
    "    for word in l:\n",
    "        if word.isalnum():\n",
    "            words.append(word)\n",
    "\n",
    "    admin_count = 0\n",
    "    disease_count = 0\n",
    "    healthcare_count = 0\n",
    "    location_count = 0\n",
    "    precaution_count = 0\n",
    "    public_count = 0\n",
    "    religion_count = 0 \n",
    "    for word in words:\n",
    "        if word in administration:\n",
    "            admin_count += 1\n",
    "        if word in disease:\n",
    "            disease_count += 1\n",
    "        if word in healthcare:\n",
    "            healthcare_count += 1\n",
    "        if word in location:\n",
    "            location_count += 1\n",
    "        if word in precaution:\n",
    "            precaution_count += 1\n",
    "        if word in public:\n",
    "            public_count += 1\n",
    "        if word in religion:\n",
    "            religion_count += 1\n",
    "\n",
    "    df2[str(ww)+'Sep'] = [admin_count, disease_count, healthcare_count, location_count, precaution_count, public_count, religion_count]\n",
    "    df3[str(ww)+'Sep'] = df2[str(ww)+'Sep']*20000/len(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87edde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"all_months.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d459dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"all_months_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5e6da58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1.1</th>\n",
       "      <th>buckets</th>\n",
       "      <th>1Apr</th>\n",
       "      <th>2Apr</th>\n",
       "      <th>3Apr</th>\n",
       "      <th>4Apr</th>\n",
       "      <th>5Apr</th>\n",
       "      <th>6Apr</th>\n",
       "      <th>7Apr</th>\n",
       "      <th>...</th>\n",
       "      <th>21Sep</th>\n",
       "      <th>22Sep</th>\n",
       "      <th>23Sep</th>\n",
       "      <th>24Sep</th>\n",
       "      <th>25Sep</th>\n",
       "      <th>26Sep</th>\n",
       "      <th>27Sep</th>\n",
       "      <th>28Sep</th>\n",
       "      <th>29Sep</th>\n",
       "      <th>30Sep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>administration</td>\n",
       "      <td>111.595090</td>\n",
       "      <td>83.426443</td>\n",
       "      <td>109.501285</td>\n",
       "      <td>132.645542</td>\n",
       "      <td>105.638541</td>\n",
       "      <td>135.006237</td>\n",
       "      <td>91.450447</td>\n",
       "      <td>...</td>\n",
       "      <td>109.417040</td>\n",
       "      <td>123.466317</td>\n",
       "      <td>71.466586</td>\n",
       "      <td>99.849433</td>\n",
       "      <td>99.594673</td>\n",
       "      <td>103.884861</td>\n",
       "      <td>83.965330</td>\n",
       "      <td>214.824850</td>\n",
       "      <td>101.979086</td>\n",
       "      <td>123.797944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>disease</td>\n",
       "      <td>831.563411</td>\n",
       "      <td>693.482309</td>\n",
       "      <td>808.223770</td>\n",
       "      <td>957.995578</td>\n",
       "      <td>967.568295</td>\n",
       "      <td>884.877834</td>\n",
       "      <td>709.627109</td>\n",
       "      <td>...</td>\n",
       "      <td>835.874439</td>\n",
       "      <td>1015.510456</td>\n",
       "      <td>725.309815</td>\n",
       "      <td>675.172359</td>\n",
       "      <td>933.410539</td>\n",
       "      <td>891.678390</td>\n",
       "      <td>880.281690</td>\n",
       "      <td>1316.044126</td>\n",
       "      <td>978.307839</td>\n",
       "      <td>1085.442688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>201.591130</td>\n",
       "      <td>145.251397</td>\n",
       "      <td>203.359529</td>\n",
       "      <td>214.759448</td>\n",
       "      <td>222.715651</td>\n",
       "      <td>238.462103</td>\n",
       "      <td>194.243584</td>\n",
       "      <td>...</td>\n",
       "      <td>132.735426</td>\n",
       "      <td>132.726291</td>\n",
       "      <td>127.727515</td>\n",
       "      <td>117.283461</td>\n",
       "      <td>182.976259</td>\n",
       "      <td>134.184612</td>\n",
       "      <td>92.091008</td>\n",
       "      <td>292.239210</td>\n",
       "      <td>126.177513</td>\n",
       "      <td>198.960982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>location</td>\n",
       "      <td>101.515533</td>\n",
       "      <td>88.640596</td>\n",
       "      <td>102.797125</td>\n",
       "      <td>133.347370</td>\n",
       "      <td>124.478536</td>\n",
       "      <td>108.591973</td>\n",
       "      <td>90.032610</td>\n",
       "      <td>...</td>\n",
       "      <td>75.336323</td>\n",
       "      <td>100.316382</td>\n",
       "      <td>76.028283</td>\n",
       "      <td>49.132261</td>\n",
       "      <td>152.866242</td>\n",
       "      <td>99.556325</td>\n",
       "      <td>73.131094</td>\n",
       "      <td>127.733695</td>\n",
       "      <td>108.892922</td>\n",
       "      <td>126.008622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>precaution</td>\n",
       "      <td>209.510782</td>\n",
       "      <td>154.934823</td>\n",
       "      <td>223.472010</td>\n",
       "      <td>238.621609</td>\n",
       "      <td>215.314224</td>\n",
       "      <td>195.172060</td>\n",
       "      <td>177.938466</td>\n",
       "      <td>...</td>\n",
       "      <td>179.372197</td>\n",
       "      <td>219.152712</td>\n",
       "      <td>173.344484</td>\n",
       "      <td>166.415722</td>\n",
       "      <td>233.931673</td>\n",
       "      <td>212.098258</td>\n",
       "      <td>203.141928</td>\n",
       "      <td>315.463518</td>\n",
       "      <td>238.527353</td>\n",
       "      <td>252.017243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>public</td>\n",
       "      <td>323.265776</td>\n",
       "      <td>233.147114</td>\n",
       "      <td>283.809453</td>\n",
       "      <td>282.836790</td>\n",
       "      <td>300.767057</td>\n",
       "      <td>325.042189</td>\n",
       "      <td>255.919467</td>\n",
       "      <td>...</td>\n",
       "      <td>252.914798</td>\n",
       "      <td>322.555753</td>\n",
       "      <td>209.838060</td>\n",
       "      <td>196.529044</td>\n",
       "      <td>287.203243</td>\n",
       "      <td>240.233741</td>\n",
       "      <td>238.353196</td>\n",
       "      <td>350.299981</td>\n",
       "      <td>269.639616</td>\n",
       "      <td>320.548248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>religion</td>\n",
       "      <td>9.359588</td>\n",
       "      <td>9.683426</td>\n",
       "      <td>7.449067</td>\n",
       "      <td>7.018283</td>\n",
       "      <td>6.055713</td>\n",
       "      <td>8.804755</td>\n",
       "      <td>4.253509</td>\n",
       "      <td>...</td>\n",
       "      <td>1.793722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.041131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.328536</td>\n",
       "      <td>2.708559</td>\n",
       "      <td>3.870718</td>\n",
       "      <td>5.185377</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1.1.1.1  Unnamed: 0.1.1.1.1.1         buckets        1Apr  \\\n",
       "0                   0                     0  administration  111.595090   \n",
       "1                   1                     1         disease  831.563411   \n",
       "2                   2                     2      healthcare  201.591130   \n",
       "3                   3                     3        location  101.515533   \n",
       "4                   4                     4      precaution  209.510782   \n",
       "5                   5                     5          public  323.265776   \n",
       "6                   6                     6        religion    9.359588   \n",
       "\n",
       "         2Apr        3Apr        4Apr        5Apr        6Apr        7Apr  \\\n",
       "0   83.426443  109.501285  132.645542  105.638541  135.006237   91.450447   \n",
       "1  693.482309  808.223770  957.995578  967.568295  884.877834  709.627109   \n",
       "2  145.251397  203.359529  214.759448  222.715651  238.462103  194.243584   \n",
       "3   88.640596  102.797125  133.347370  124.478536  108.591973   90.032610   \n",
       "4  154.934823  223.472010  238.621609  215.314224  195.172060  177.938466   \n",
       "5  233.147114  283.809453  282.836790  300.767057  325.042189  255.919467   \n",
       "6    9.683426    7.449067    7.018283    6.055713    8.804755    4.253509   \n",
       "\n",
       "   ...       21Sep        22Sep       23Sep       24Sep       25Sep  \\\n",
       "0  ...  109.417040   123.466317   71.466586   99.849433   99.594673   \n",
       "1  ...  835.874439  1015.510456  725.309815  675.172359  933.410539   \n",
       "2  ...  132.735426   132.726291  127.727515  117.283461  182.976259   \n",
       "3  ...   75.336323   100.316382   76.028283   49.132261  152.866242   \n",
       "4  ...  179.372197   219.152712  173.344484  166.415722  233.931673   \n",
       "5  ...  252.914798   322.555753  209.838060  196.529044  287.203243   \n",
       "6  ...    1.793722     0.000000    3.041131    0.000000    0.000000   \n",
       "\n",
       "        26Sep       27Sep        28Sep       29Sep        30Sep  \n",
       "0  103.884861   83.965330   214.824850  101.979086   123.797944  \n",
       "1  891.678390  880.281690  1316.044126  978.307839  1085.442688  \n",
       "2  134.184612   92.091008   292.239210  126.177513   198.960982  \n",
       "3   99.556325   73.131094   127.733695  108.892922   126.008622  \n",
       "4  212.098258  203.141928   315.463518  238.527353   252.017243  \n",
       "5  240.233741  238.353196   350.299981  269.639616   320.548248  \n",
       "6    4.328536    2.708559     3.870718    5.185377     0.000000  \n",
       "\n",
       "[7 rows x 186 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1'], axis = 1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1640a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buckets</th>\n",
       "      <th>1Apr</th>\n",
       "      <th>2Apr</th>\n",
       "      <th>3Apr</th>\n",
       "      <th>4Apr</th>\n",
       "      <th>5Apr</th>\n",
       "      <th>6Apr</th>\n",
       "      <th>7Apr</th>\n",
       "      <th>8Apr</th>\n",
       "      <th>9Apr</th>\n",
       "      <th>...</th>\n",
       "      <th>21Sep</th>\n",
       "      <th>22Sep</th>\n",
       "      <th>23Sep</th>\n",
       "      <th>24Sep</th>\n",
       "      <th>25Sep</th>\n",
       "      <th>26Sep</th>\n",
       "      <th>27Sep</th>\n",
       "      <th>28Sep</th>\n",
       "      <th>29Sep</th>\n",
       "      <th>30Sep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>administration</td>\n",
       "      <td>111.595090</td>\n",
       "      <td>83.426443</td>\n",
       "      <td>109.501285</td>\n",
       "      <td>132.645542</td>\n",
       "      <td>105.638541</td>\n",
       "      <td>135.006237</td>\n",
       "      <td>91.450447</td>\n",
       "      <td>105.671771</td>\n",
       "      <td>127.882193</td>\n",
       "      <td>...</td>\n",
       "      <td>109.417040</td>\n",
       "      <td>123.466317</td>\n",
       "      <td>71.466586</td>\n",
       "      <td>99.849433</td>\n",
       "      <td>99.594673</td>\n",
       "      <td>103.884861</td>\n",
       "      <td>83.965330</td>\n",
       "      <td>214.824850</td>\n",
       "      <td>101.979086</td>\n",
       "      <td>123.797944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disease</td>\n",
       "      <td>831.563411</td>\n",
       "      <td>693.482309</td>\n",
       "      <td>808.223770</td>\n",
       "      <td>957.995578</td>\n",
       "      <td>967.568295</td>\n",
       "      <td>884.877834</td>\n",
       "      <td>709.627109</td>\n",
       "      <td>803.680541</td>\n",
       "      <td>950.203449</td>\n",
       "      <td>...</td>\n",
       "      <td>835.874439</td>\n",
       "      <td>1015.510456</td>\n",
       "      <td>725.309815</td>\n",
       "      <td>675.172359</td>\n",
       "      <td>933.410539</td>\n",
       "      <td>891.678390</td>\n",
       "      <td>880.281690</td>\n",
       "      <td>1316.044126</td>\n",
       "      <td>978.307839</td>\n",
       "      <td>1085.442688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>healthcare</td>\n",
       "      <td>201.591130</td>\n",
       "      <td>145.251397</td>\n",
       "      <td>203.359529</td>\n",
       "      <td>214.759448</td>\n",
       "      <td>222.715651</td>\n",
       "      <td>238.462103</td>\n",
       "      <td>194.243584</td>\n",
       "      <td>196.247574</td>\n",
       "      <td>227.087774</td>\n",
       "      <td>...</td>\n",
       "      <td>132.735426</td>\n",
       "      <td>132.726291</td>\n",
       "      <td>127.727515</td>\n",
       "      <td>117.283461</td>\n",
       "      <td>182.976259</td>\n",
       "      <td>134.184612</td>\n",
       "      <td>92.091008</td>\n",
       "      <td>292.239210</td>\n",
       "      <td>126.177513</td>\n",
       "      <td>198.960982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>location</td>\n",
       "      <td>101.515533</td>\n",
       "      <td>88.640596</td>\n",
       "      <td>102.797125</td>\n",
       "      <td>133.347370</td>\n",
       "      <td>124.478536</td>\n",
       "      <td>108.591973</td>\n",
       "      <td>90.032610</td>\n",
       "      <td>99.920926</td>\n",
       "      <td>123.231932</td>\n",
       "      <td>...</td>\n",
       "      <td>75.336323</td>\n",
       "      <td>100.316382</td>\n",
       "      <td>76.028283</td>\n",
       "      <td>49.132261</td>\n",
       "      <td>152.866242</td>\n",
       "      <td>99.556325</td>\n",
       "      <td>73.131094</td>\n",
       "      <td>127.733695</td>\n",
       "      <td>108.892922</td>\n",
       "      <td>126.008622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precaution</td>\n",
       "      <td>209.510782</td>\n",
       "      <td>154.934823</td>\n",
       "      <td>223.472010</td>\n",
       "      <td>238.621609</td>\n",
       "      <td>215.314224</td>\n",
       "      <td>195.172060</td>\n",
       "      <td>177.938466</td>\n",
       "      <td>185.464740</td>\n",
       "      <td>225.537686</td>\n",
       "      <td>...</td>\n",
       "      <td>179.372197</td>\n",
       "      <td>219.152712</td>\n",
       "      <td>173.344484</td>\n",
       "      <td>166.415722</td>\n",
       "      <td>233.931673</td>\n",
       "      <td>212.098258</td>\n",
       "      <td>203.141928</td>\n",
       "      <td>315.463518</td>\n",
       "      <td>238.527353</td>\n",
       "      <td>252.017243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>public</td>\n",
       "      <td>323.265776</td>\n",
       "      <td>233.147114</td>\n",
       "      <td>283.809453</td>\n",
       "      <td>282.836790</td>\n",
       "      <td>300.767057</td>\n",
       "      <td>325.042189</td>\n",
       "      <td>255.919467</td>\n",
       "      <td>253.756020</td>\n",
       "      <td>330.943616</td>\n",
       "      <td>...</td>\n",
       "      <td>252.914798</td>\n",
       "      <td>322.555753</td>\n",
       "      <td>209.838060</td>\n",
       "      <td>196.529044</td>\n",
       "      <td>287.203243</td>\n",
       "      <td>240.233741</td>\n",
       "      <td>238.353196</td>\n",
       "      <td>350.299981</td>\n",
       "      <td>269.639616</td>\n",
       "      <td>320.548248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>religion</td>\n",
       "      <td>9.359588</td>\n",
       "      <td>9.683426</td>\n",
       "      <td>7.449067</td>\n",
       "      <td>7.018283</td>\n",
       "      <td>6.055713</td>\n",
       "      <td>8.804755</td>\n",
       "      <td>4.253509</td>\n",
       "      <td>7.907411</td>\n",
       "      <td>3.875218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.793722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.041131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.328536</td>\n",
       "      <td>2.708559</td>\n",
       "      <td>3.870718</td>\n",
       "      <td>5.185377</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          buckets        1Apr        2Apr        3Apr        4Apr        5Apr  \\\n",
       "0  administration  111.595090   83.426443  109.501285  132.645542  105.638541   \n",
       "1         disease  831.563411  693.482309  808.223770  957.995578  967.568295   \n",
       "2      healthcare  201.591130  145.251397  203.359529  214.759448  222.715651   \n",
       "3        location  101.515533   88.640596  102.797125  133.347370  124.478536   \n",
       "4      precaution  209.510782  154.934823  223.472010  238.621609  215.314224   \n",
       "5          public  323.265776  233.147114  283.809453  282.836790  300.767057   \n",
       "6        religion    9.359588    9.683426    7.449067    7.018283    6.055713   \n",
       "\n",
       "         6Apr        7Apr        8Apr        9Apr  ...       21Sep  \\\n",
       "0  135.006237   91.450447  105.671771  127.882193  ...  109.417040   \n",
       "1  884.877834  709.627109  803.680541  950.203449  ...  835.874439   \n",
       "2  238.462103  194.243584  196.247574  227.087774  ...  132.735426   \n",
       "3  108.591973   90.032610   99.920926  123.231932  ...   75.336323   \n",
       "4  195.172060  177.938466  185.464740  225.537686  ...  179.372197   \n",
       "5  325.042189  255.919467  253.756020  330.943616  ...  252.914798   \n",
       "6    8.804755    4.253509    7.907411    3.875218  ...    1.793722   \n",
       "\n",
       "         22Sep       23Sep       24Sep       25Sep       26Sep       27Sep  \\\n",
       "0   123.466317   71.466586   99.849433   99.594673  103.884861   83.965330   \n",
       "1  1015.510456  725.309815  675.172359  933.410539  891.678390  880.281690   \n",
       "2   132.726291  127.727515  117.283461  182.976259  134.184612   92.091008   \n",
       "3   100.316382   76.028283   49.132261  152.866242   99.556325   73.131094   \n",
       "4   219.152712  173.344484  166.415722  233.931673  212.098258  203.141928   \n",
       "5   322.555753  209.838060  196.529044  287.203243  240.233741  238.353196   \n",
       "6     0.000000    3.041131    0.000000    0.000000    4.328536    2.708559   \n",
       "\n",
       "         28Sep       29Sep        30Sep  \n",
       "0   214.824850  101.979086   123.797944  \n",
       "1  1316.044126  978.307839  1085.442688  \n",
       "2   292.239210  126.177513   198.960982  \n",
       "3   127.733695  108.892922   126.008622  \n",
       "4   315.463518  238.527353   252.017243  \n",
       "5   350.299981  269.639616   320.548248  \n",
       "6     3.870718    5.185377     0.000000  \n",
       "\n",
       "[7 rows x 184 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df3.drop(['Unnamed: 0.1.1.1.1', 'Unnamed: 0.1.1.1.1.1'], axis = 1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798dc0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
